======================================================================
GJ 1132 Ribas XUV Model -- Posterior Inference Experiment Report
======================================================================

Date:           2026-02-27
Script:         gj1132_alabi.py
Branch:         emcee
System:         Linux 6.8.0-64-generic
Host CPUs:      iNumCores = max(1, multiprocessing.cpu_count() - 1)

This report documents the complete configuration, training, and
sampling results for the Bayesian posterior inference of GJ 1132's
XUV luminosity evolution parameters using the Ribas saturation model.
The surrogate model was trained via alabi (GP active learning), then
sampled by four independent posterior samplers: emcee, dynesty,
PyMultiNest, and UltraNest.


======================================================================
1. PROBLEM DEFINITION
======================================================================

1.1 Forward Model
----------------------------------------------------------------------
The forward model is VPLanet (Virtual Planet Simulator), called via
the vplanet_inference Python wrapper. Each forward-model call evolves
a stellar model for GJ 1132 from the zero-age main sequence to a
specified age, returning the bolometric luminosity (Lbol) and XUV
luminosity (Lxuv) at the final time step.

The five free parameters (canonical ordering) are:

  Index   Parameter         Symbol          Units        Bounds
  -----   ---------         ------          -----        ------
    0     Stellar mass      m_star          M_sun        [0.17, 0.22]
    1     Sat. XUV frac.    log10(f_sat)    dex          [-4.0, -2.15]
    2     Sat. time         t_sat           Gyr          [0.1, 5.0]
    3     System age        Age             Gyr          [1.0, 13.0]
    4     XUV decay index   beta_XUV        --           [0.4, 2.1]

Number of dimensions:  iNumDimensions = 5

1.2 Observational Constraints
----------------------------------------------------------------------
Two observables constrain the likelihood:

  Observable                Mean         Sigma
  ----------                ----         -----
  Lbol [Lsun]              4.38e-3      3.4e-4
  log10(Lxuv/Lbol)        -4.26         0.15

The log-likelihood is:
  ln(L) = -0.5 * sum_i [ (model_i - obs_i)^2 / sigma_i^2 ]

1.3 Prior Distributions
----------------------------------------------------------------------
  Parameter        Prior Type             Parameters
  ---------        ----------             ----------
  m_star           Asymmetric Gaussian    mean=0.1945, sigma+=0.0048, sigma-=0.0046
  log10(f_sat)     Symmetric Gaussian     mean=-2.92, sigma=0.26
  t_sat            Uniform                (no constraint)
  Age              Symmetric Gaussian     mean=5.75, sigma=1.38
  beta_XUV         Symmetric Gaussian     mean=1.18, sigma=0.31

The log-prior evaluates as:
  ln(pi) = sum_i [ -0.5 * ((theta_i - mu_i) / sigma_i)^2 ]
where sigma_i depends on which side of the mean (for asymmetric case),
and uniform priors contribute 0 within bounds (-inf outside).

1.4 Maximum A Posteriori (MAP) Estimate (from MaxLEV)
----------------------------------------------------------------------
MAP results file: maxlike_results.txt

  Parameter         MAP Value
  ---------         ---------
  m_star            0.1940615
  log10(f_sat)     -2.919368
  t_sat             0.4190606
  Age               5.754856
  beta_XUV          1.179282

  -ln(Posterior)  = 7.665002e-03
  -ln(Likelihood) = 3.110497e-03
  -ln(Prior)      = 4.554504e-03
  chi^2           = 6.220995e-03

Model predictions at MAP:
  L_bol             = 4.406586e-3 Lsun  (obs: 4.380e-3, residual: +0.078 sigma)
  log(Lxuv/Lbol)   = -4.261550       (obs: -4.260,   residual: -0.010 sigma)

The MAP point is essentially a perfect fit (total chi^2 = 0.006).

1.5 Likelihood Floor
----------------------------------------------------------------------
The likelihood function returns -1e2 (not -1e10) for failed VPLanet
runs or non-physical outputs:
  - VPLanet exceptions (crashed simulations)
  - Non-finite Lbol or Lxuv
  - Negative Lbol or Lxuv

The floor was changed from -1e10 to -1e2 because:
  (a) The GP surrogate uses a scaler on the y-values; extreme floor
      values compress the dynamic range and reduce surrogate accuracy.
  (b) The actual minimum log-likelihood in the viable region is roughly
      -50, so -1e2 is already well below any physical solution.
  (c) A very negative floor creates numerical challenges for nested
      samplers that must integrate over the full prior volume.

Approximately 77% of parameter space evaluations hit this floor,
reflecting the narrow region of parameter space that produces
physically valid stellar models.


======================================================================
2. GP SURROGATE TRAINING
======================================================================

2.1 SurrogateModel Initialization
----------------------------------------------------------------------
  SurrogateModel(
      lnlike_fn   = fdLogLikelihood,
      bounds      = listBounds,      # 5 dimensions
      savedir     = "output/",
      cache       = True,
      verbose     = True,
      show_warnings = True,
      ncore       = iNumCores,
      pool_method = "fork",          # see Section 7.3
  )

2.2 Initial Samples
----------------------------------------------------------------------
  Method:           Latin Hypercube Sampling (LHS)
  iNumTraining:     500   (= 100 * 5 dimensions)
  iNumTest:         500   (= 100 * 5 dimensions)
  Total initial:    1000 likelihood evaluations

Files:
  output/initial_train_file_sample.npz
  output/initial_test_sample.npz

2.3 Base GP Configuration (dictBaseGPConfig)
----------------------------------------------------------------------
  fit_amp:          True
  fit_mean:         True
  fit_white_noise:  False
  white_noise:      -12
  uniform_scales:   False
  hyperopt_method:  "ml"         (maximum likelihood)
  gp_opt_method:    "l-bfgs-b"
  gp_scale_rng:     [-2, 6]
  gp_amp_rng:       [-1, 1]
  multi_proc:       True

2.4 Grid Search (36 Combinations)
----------------------------------------------------------------------
Search grid:
  kernel:        [ExpSquaredKernel, Matern32Kernel, Matern52Kernel]
  theta_scaler:  [no_scaler, MinMaxScaler, StandardScaler]
  y_scaler:      [no_scaler, nlog_scaler, MinMaxScaler, StandardScaler]

Total combinations: 3 kernels x 3 theta_scalers x 4 y_scalers = 36

Full results (ordered by grid position):

  #    Kernel              theta_scaler    y_scaler         MSE          Note
  ---  ------              ------------    --------         ---          ----
   1   ExpSquaredKernel    no_scaler       no_scaler        6.4700e+01   first best
   2   ExpSquaredKernel    no_scaler       nlog_scaler      1.3099e+02
   3   ExpSquaredKernel    no_scaler       MinMaxScaler     6.3429e+00   best so far
   4   ExpSquaredKernel    no_scaler       StandardScaler   8.2416e+00
   5   ExpSquaredKernel    MinMaxScaler    no_scaler        4.8374e+01
   6   ExpSquaredKernel    MinMaxScaler    nlog_scaler      1.4964e+02
   7   ExpSquaredKernel    MinMaxScaler    MinMaxScaler     7.0366e+00
   8   ExpSquaredKernel    MinMaxScaler    StandardScaler   1.0652e+01
   9   ExpSquaredKernel    StandardScaler  no_scaler        4.8685e+01
  10   ExpSquaredKernel    StandardScaler  nlog_scaler      1.5154e+02
  11   ExpSquaredKernel    StandardScaler  MinMaxScaler     6.9120e+00
  12   ExpSquaredKernel    StandardScaler  StandardScaler   1.0343e+01
  13   Matern32Kernel      no_scaler       no_scaler        3.5752e+01
  14   Matern32Kernel      no_scaler       nlog_scaler      3.8438e+01
  15   Matern32Kernel      no_scaler       MinMaxScaler     8.7400e+00
  16   Matern32Kernel      no_scaler       StandardScaler   1.0474e+01
  17   Matern32Kernel      MinMaxScaler    no_scaler        3.9136e+01
  18   Matern32Kernel      MinMaxScaler    nlog_scaler      4.5967e+01
  19   Matern32Kernel      MinMaxScaler    MinMaxScaler     9.3904e+00
  20   Matern32Kernel      MinMaxScaler    StandardScaler   9.9626e+00
  21   Matern32Kernel      StandardScaler  no_scaler        3.9117e+01
  22   Matern32Kernel      StandardScaler  nlog_scaler      4.5477e+01
  23   Matern32Kernel      StandardScaler  MinMaxScaler     1.0008e+01
  24   Matern32Kernel      StandardScaler  StandardScaler   9.9579e+00
  25   Matern52Kernel      no_scaler       no_scaler        4.0441e+01
  26   Matern52Kernel      no_scaler       nlog_scaler      4.1965e+01   (optimizer failed)
  27   Matern52Kernel      no_scaler       MinMaxScaler     3.8845e+00   best so far
  28   Matern52Kernel      no_scaler       StandardScaler   5.1930e+00
  29   Matern52Kernel      MinMaxScaler    no_scaler        3.8778e+01
  30   Matern52Kernel      MinMaxScaler    nlog_scaler      6.8875e+01
  31   Matern52Kernel      MinMaxScaler    MinMaxScaler     3.8883e+00
  32   Matern52Kernel      MinMaxScaler    StandardScaler   5.1926e+00
  33   Matern52Kernel      StandardScaler  no_scaler        3.8420e+01
  34   Matern52Kernel      StandardScaler  nlog_scaler      6.7853e+01
  35   Matern52Kernel      StandardScaler  MinMaxScaler     3.8789e+00   WINNER
  36   Matern52Kernel      StandardScaler  StandardScaler   5.2139e+00

Winning configuration:
  Kernel:          Matern52Kernel
  theta_scaler:    StandardScaler (sklearn.preprocessing)
  y_scaler:        MinMaxScaler   (sklearn.preprocessing)
  Best test MSE:   3.8789e+00

Key observations:
  - MinMaxScaler for y consistently outperforms other y_scalers
  - Matern-5/2 outperforms both ExpSquared and Matern-3/2
  - The top 5 configs are all Matern-5/2 with MinMaxScaler for y
  - nlog_scaler consistently produces the worst results
  - theta_scaler choice has minimal impact within a kernel family

2.5 Active Learning Configuration (dictActiveLearningConfig)
----------------------------------------------------------------------
  algorithm:          "bape"   (Bayesian Active Learning by
                                Posterior Estimation; Kandasamy+2017)
  gp_opt_freq:        50      (re-optimize GP hyperparams every 50 iters)
  obj_opt_method:     "nelder-mead"
  use_grad_opt:       False
  nopt:               1       (number of optimization restarts)
  optimizer_kwargs:
      max_iter:       200
      xatol:          1e-3
      fatol:          1e-2
      adaptive:       True

ML optimization config (dictMLOptConfig, used during resume):
  hyperopt_method:    "ml"
  optimizer_kwargs:
      maxiter:        100
      xatol:          1e-4
      fatol:          1e-3
      adaptive:       True

2.6 Active Learning Results
----------------------------------------------------------------------
  Total iterations:       500
  Total training points:  1000  (500 initial + 500 active)
  Test set size:          500   (held out, never trained on)

Test MSE progression during active learning (at GP re-optimization
points, every 50 iterations):

  Iteration    Test MSE
  ---------    --------
  (initial)    3.8789      (from grid search winner)
  50           3.6231
  100          3.5739
  150          3.5803
  200          3.4605
  250          3.1402
  300          3.1082
  350          3.0534
  400          3.0721
  450          3.0936
  500          3.0978      (final)

The test MSE decreased from 3.88 to 3.10 during active learning,
a 20.1% reduction. The MSE stabilized after approximately 300
iterations, indicating convergence of the surrogate.

Active learning wall time: approximately 10 minutes (500 iterations
at ~1.1 seconds per iteration).

2.7 Final GP Hyperparameters
----------------------------------------------------------------------
From surrogate_model.txt:

  [mean:value]                       0.7171609514163187
  [kernel:k1:log_constant]           2.302585092994046
  [kernel:k2:metric:log_M_0_0]      7.7145840525445495  (m_star)
  [kernel:k2:metric:log_M_1_1]      5.986214286553181   (f_sat)
  [kernel:k2:metric:log_M_2_2]      1.9551460149223496  (t_sat)
  [kernel:k2:metric:log_M_3_3]      4.043624836553096   (Age)
  [kernel:k2:metric:log_M_4_4]      5.148126079057132   (beta_XUV)

Hyperparameter bounds:
  [mean:value]          [0.717, 1.020]
  [log_constant]        [-2.303, 2.303]
  [log_M_i_i]           [-4.605, 13.816]   (for all 5 dimensions)

The length-scale parameters reveal the GP's sensitivity:
  - log_M_0_0 = 7.71 (m_star):    longest scale => weakest variation
  - log_M_1_1 = 5.99 (f_sat):     moderate scale
  - log_M_2_2 = 1.96 (t_sat):     shortest scale => strongest variation
  - log_M_3_3 = 4.04 (Age):       moderate scale
  - log_M_4_4 = 5.15 (beta_XUV):  moderate scale

The t_sat dimension has the shortest length scale, meaning the
likelihood varies most rapidly with saturation time. This is
consistent with the physics: t_sat directly controls the transition
point in the XUV evolution, creating a sharp feature in the
likelihood surface.


======================================================================
3. SAMPLER CONFIGURATIONS
======================================================================

3.1 Emcee Configuration (dictEmceeConfig)
----------------------------------------------------------------------
  nwalkers:              100   (= 20 * 5 dimensions)
  nsteps:                20000 (= 2e4)
  min_ess:               10000

  Proposal moves:
      DEMove:            weight 0.8 (differential evolution)
      DESnookerMove:     weight 0.2 (snooker proposal)

  multi_proc:            False  (serial evaluation)
  samples_file:          "emcee_samples.npz"

  Walker initialization:
      Method:            MAP-centred Gaussian perturbation
      MAP source:        MaxLEV results
      Scatter:           1% of bound width per dimension
      Clipping:          walkers clipped to stay within bounds

  Surrogate likelihood:
      return_var:        False  (mean-only prediction)
      Wrapper:           ffnSafeSurrogate (clamps non-finite and >0 to -1e4)

3.2 Dynesty Configuration (dictDynestyConfig)
----------------------------------------------------------------------
  Mode:                  "dynamic" (DynamicNestedSampler)
  sampler_kwargs:
      bound:             "multi"     (multi-ellipsoidal decomposition)
      nlive:             50          (= 10 * 5 dimensions)
      sample:            "rslice"    (random slice sampling)
      bootstrap:         0
  run_kwargs:
      dlogz_init:        1.0         (relaxed initial evidence tolerance)
      print_progress:    True
  Default run_kwargs (from alabi, merged with above):
      wt_kwargs:         {"pfrac": 1.0}  (focus on posterior, not evidence)
      stop_kwargs:       {"pfrac": 1.0}
      maxiter:           50000
  min_ess:               10000
  multi_proc:            False

  Prior transform:
      Function:          ut.prior_transform_normal
      Maps:              [0,1]^5 -> parameter space via inverse CDF
      Gaussian params:   uses listPriorData (Gaussian for constrained params,
                         uniform for t_sat)

  Surrogate likelihood:
      return_var:        False  (mean-only)
      Wrapper:           ffnSafeSurrogate (clamp floor = -1e4)

3.3 PyMultiNest Configuration (dictMultinestConfig)
----------------------------------------------------------------------
  sampler_kwargs:
      n_live_points:     500          (= 100 * 5 dimensions)
      sampling_efficiency: 0.8
      evidence_tolerance: 0.5
  Default sampler_kwargs (from alabi, merged with above):
      n_iter_before_update: 100
      null_log_evidence:  -1e90
      max_modes:          100
      mode_tolerance:     -1e90
      seed:               -1         (random)
      verbose:            True
      importance_nested_sampling: True
      multimodal:         True
      const_efficiency_mode: False
  min_ess:               10000
  multi_proc:            False

  Prior transform:
      Function:          ut.prior_transform_normal (wrapped for PyMultiNest
                         in-place cube convention)
      Maps:              [0,1]^5 -> parameter space via inverse CDF
  Output prefix:         pymultinest_custom_iter_500__runNN

3.4 UltraNest Configuration (dictUltranestConfig)
----------------------------------------------------------------------
  run_kwargs:
      min_num_live_points: 500       (= 100 * 5 dimensions)
      dlogz:             0.5
      dKL:               0.5
      frac_remain:       0.01
      show_status:       True
  Default run_kwargs (from alabi, merged with above):
      update_interval_volume_fraction: 0.8
      viz_callback:      False
      Lepsilon:          0.001
      min_ess:           400         (UltraNest's internal ESS target)
      max_num_improvement_loops: 1
      cluster_num_live_points: 40
      insertion_test_zscore_threshold: 4
      insertion_test_window: 10
      widen_before_initial_plateau_num_warn: 10000
  Default sampler_kwargs (from alabi):
      derived_param_names: []
      wrapped_params:    None
      num_test_samples:  2
      draw_multiple:     True
      num_bootstraps:    30
      vectorized:        False
      ndraw_min:         128
      ndraw_max:         65536
      storage_backend:   "hdf5"
      warmstart_max_tau: -1
  min_ess:               10000
  resume:                "overwrite"
  log_dir:               output/ultranest_custom_iter_500


======================================================================
4. HOW ALABI MANAGES THE SAMPLERS (from core.py)
======================================================================

4.1 Multi-Run Accumulation Logic (All Samplers)
----------------------------------------------------------------------
All four samplers share a common multi-run architecture:

  1. accumulated_samples = 0; run_number = 1
  2. while accumulated_samples < iMinSamples:
       a. Run sampler for one complete cycle
       b. Extract/resample posterior samples
       c. accumulated_samples += current_nsamples
       d. If accumulated_samples >= iMinSamples: break
       e. run_number += 1
       f. If run_number > 10: break with warning
  3. Combine all runs' samples via np.vstack()

  iMinSamples = max(min_ess, 1)

This means the target is the RAW number of (post-processed) samples,
not the effective sample size computed via autocorrelation analysis.
For emcee, the "samples" are already thinned and burn-in removed.
For nested samplers, the "samples" are equally-weighted resampled
draws from the weighted posterior.

4.2 Emcee: ESS Computation and Burn-in/Thinning
----------------------------------------------------------------------
After each run:
  1. mcmc_utils.estimate_burnin(sampler) is called
  2. Autocorrelation time tau is computed: sampler.get_autocorr_time(tol=0)
  3. Burn-in: iburn = int(2.0 * max(tau))
  4. Thinning: ithin = max(int(0.5 * min(tau)), 1)
  5. Samples extracted: sampler.get_chain(discard=iburn, thin=ithin, flat=True)

After all runs combined:
  - acc_frac = mean(sampler.acceptance_fraction)   (from final run)
  - autcorr_time = mean(sampler.get_autocorr_time())  (from final run)
    Falls back to get_autocorr_time(quiet=True) if exception raised.

The ESS check in the while-loop counts post-burn-in, thinned samples,
not raw chain length. This means the 10000 min_ess target is applied
to already-processed samples.

For subsequent runs (run_number > 1):
  - p0 is set from sampler.get_last_sample().coords (final walker
    positions from previous run become initial positions for next run)

4.3 Dynesty: ESS Computation and Evidence
----------------------------------------------------------------------
After each run:
  1. Results extracted: dsampler.results
  2. Weights computed: exp(logwt - logz[-1])
  3. Resampled: dyfunc.resample_equal(samples, weights)
  4. accumulated_samples counts resampled (equally-weighted) draws

For multiple runs:
  - All resampled draws are vstacked
  - Best logZ (highest) is kept: self.dynesty_logz = max(all_logz)
  - logZ error from final run: current_results.logzerr[-1]

Dynesty does not compute autocorrelation-based ESS; the resampled
count IS the effective sample size (since weights are equalized).

For the DynamicNestedSampler mode used here:
  - Default run_kwargs include wt_kwargs={'pfrac': 1.0} and
    stop_kwargs={'pfrac': 1.0}, which focus entirely on posterior
    sampling rather than evidence estimation.

4.4 PyMultiNest: ESS Computation and Evidence
----------------------------------------------------------------------
After each run:
  1. Output files fixed for malformed scientific notation
  2. Analyzer reads equal-weighted posterior samples
  3. Samples extracted: analyzer.get_equal_weighted_posterior()[:, :-1]
  4. Evidence: analyzer.get_stats()['global evidence'] (NSE value)
  5. Evidence error: analyzer.get_stats()['global evidence error']

For multiple runs:
  - All equal-weighted samples vstacked
  - Combined logZ: weighted average (weights = sample counts per run)
  - Combined logZ_err: sqrt of weighted average of squared errors

Each run gets a unique output basename (run01, run02, ...) so
MultiNest starts fresh each time (not resuming).

4.5 UltraNest: ESS Computation and Evidence
----------------------------------------------------------------------
After each run:
  1. Results: sampler.run(**final_run_kwargs)
  2. Samples: current_results['samples'] (equally weighted)
  3. logZ: current_results['logz']
  4. logZ_err: current_results['logzerr']

UltraNest reports its own internal ESS as part of the results
(stored in results['ess']), but alabi uses the raw sample count
for the multi-run accumulation check.

For multiple runs:
  - All samples vstacked
  - Best logZ (highest) is kept
  - Each run gets a separate log_dir (run2, run3, ...)
  - First run uses resume="overwrite", subsequent runs always overwrite


======================================================================
5. RUNTIME STATISTICS
======================================================================

5.1 Emcee
----------------------------------------------------------------------
  Number of runs:              1
  Walkers:                     100
  Steps per walker:            20000
  Total raw chain:             100 * 20000 = 2,000,000 samples
  Burn-in estimate:            69 steps
  Thinning estimate:           11 steps
  Effective chain per walker:  (20000 - 69) / 11 = 1811 samples
  Total post-processed:        181,100 samples
  Wall time:                   3 min 41 sec (221 seconds)
  Throughput:                  ~90 iterations/second

  Mean acceptance fraction:    0.227
  Mean autocorrelation time:   26.853 steps

  Note: Only 1 run was needed because 181,100 >> 10,000 (min_ess).

5.2 Dynesty
----------------------------------------------------------------------
  Mode:                        DynamicNestedSampler
  Number of runs:              1
  Live points:                 50
  Total batches:               71
  Total iterations:            10,298
  Total likelihood calls:      326,801
  Sampling efficiency:         3.14%
  Final logZ:                  -2.867 +/- 0.044
  Total post-processed:        10,298 equally-weighted samples
  Wall time:                   ~89 seconds

  Note: Only 1 run was needed (10,298 > 10,000).
  The dynamic sampler allocated 71 batches, adaptively refining the
  posterior. The low sampling efficiency (3.14%) reflects the narrow
  likelihood region embedded in a broad prior volume.

5.3 PyMultiNest
----------------------------------------------------------------------
  Number of runs:              7
  Live points per run:         500
  Evidence tolerance:          0.5
  Sampling efficiency:         0.8

  Run-by-run breakdown:

  Run   Samples   Nested ln(Z)    INS ln(Z)           Likelihood Evals
  ---   -------   ------------    ---------           ----------------
   1    1,583     -2.994          -3.527 +/- 0.031    4,087
   2    1,581     -2.956          -3.506 +/- 0.034    (similar)
   3    1,633     -3.111          -3.558 +/- 0.037    (similar)
   4    1,582     -3.059          -3.544 +/- 0.032    (similar)
   5    1,570     -3.026          -3.573 +/- 0.036    (similar)
   6    1,583     -2.987          -3.419 +/- 0.046    (similar)
   7    1,580     -3.128          -3.584 +/- 0.040    4,079

  Combined total:              11,112 equally-weighted samples
  Combined log evidence:       -3.530 +/- 0.037
  Total wall time:             24.54 seconds
  Average per run:             ~3.5 seconds

  Note: 7 runs were required because each run produced ~1,580 samples,
  well below the 10,000 target. The consistent per-run sample count
  reflects MultiNest's deterministic termination at the specified
  evidence tolerance.

  Per-run Nested Sampling (NS) vs Importance Nested Sampling (INS):
  The INS evidence is consistently more negative than the NS evidence,
  which is expected because INS provides a more conservative estimate.

5.4 UltraNest
----------------------------------------------------------------------
  Number of runs:              3
  Minimum live points:         500
  Convergence criteria:        dlogz=0.5, dKL=0.5, frac_remain=0.01

  Run-by-run breakdown:

  Run   Samples   logZ               ESS      H (nats)     ncall
  ---   -------   ----               ---      --------     -----
   1    4,370     -3.151 +/- 0.096   1,812    2.201+/-0.044  222,457
   2    4,345     -3.100 +/- 0.139   1,836    2.134+/-0.051  428,477
   3    4,302     -3.013 +/- 0.150   1,802    2.072+/-0.045  230,299

  Combined total:              13,017 equally-weighted samples
  Best log evidence:           -3.013 +/- 0.150
  Total wall time:             335.54 seconds (~5.6 minutes)
  Total likelihood calls:      881,233

  Max-likelihood points found by UltraNest:
    Run 1: logl = -4.534e-05, point = [0.1946, -2.930, 0.294, 4.276, 1.208]
    Run 2: logl = -7.605e-06, point = [0.1915, -3.319, 0.784, 7.212, 0.918]
    Run 3: logl = -1.110e-04, point = [0.1911, -3.307, 0.758, 7.326, 0.946]

  The three runs show consistent information content (H ~ 2.1 nats)
  and ESS per run (~1800), reflecting a stable posterior shape.


======================================================================
6. FINAL PARAMETER ESTIMATES
======================================================================

6.1 Posterior Summary Table (mean +/- std)
----------------------------------------------------------------------

Parameter             Emcee              Dynesty            MultiNest          UltraNest
---------             -----              -------            ---------          ---------
m_star [Msun]         0.194021+/-0.003994  0.194015+/-0.004252  0.193863+/-0.004026  0.193976+/-0.004280
log10(f_sat) [dex]   -3.049121+/-0.243869 -3.055125+/-0.259531 -3.044106+/-0.232450 -3.063876+/-0.256821
t_sat [Gyr]           0.824931+/-0.525741  0.879568+/-0.594840  0.847783+/-0.513776  0.883406+/-0.606243
Age [Gyr]             6.090237+/-1.246761  6.117638+/-1.282016  6.165435+/-1.169893  6.121285+/-1.295040
beta_XUV              1.276392+/-0.259145  1.289641+/-0.270401  1.291709+/-0.240877  1.285141+/-0.279723

Sample counts:
  Emcee:      181,100 samples
  Dynesty:     10,298 samples
  MultiNest:   11,112 samples
  UltraNest:   13,017 samples

6.2 Log-Evidence Comparison
----------------------------------------------------------------------

  Sampler        ln(Z)              Method
  -------        -----              ------
  Dynesty       -2.867 +/- 0.044   Dynamic nested sampling (71 batches)
  MultiNest     -3.530 +/- 0.037   Importance nested sampling (7 runs combined)
  UltraNest     -3.013 +/- 0.150   Reactive nested sampling (3 runs, best)
  Emcee          N/A                MCMC (no evidence estimate)

Note: The evidence values differ because:
  (a) Dynesty uses DynamicNestedSampler with pfrac=1.0, which
      focuses on posterior sampling and may be less accurate for Z.
  (b) MultiNest's INS evidence is generally more conservative.
  (c) UltraNest's values vary between runs (range: -3.15 to -3.01).
  (d) The narrow posterior relative to the prior makes evidence
      estimation inherently challenging.

6.3 Pairwise Delta/Sigma Agreement
----------------------------------------------------------------------
The delta/sigma statistic is defined as:
  delta/sigma = |mean_A - mean_B| / ((sigma_A + sigma_B) / 2)

where sigma is the posterior standard deviation.

m_star [Msun]:
  Emcee vs Dynesty:     0.00 (excellent)
  Emcee vs MultiNest:   0.04 (excellent)
  Emcee vs UltraNest:   0.01 (excellent)
  Dynesty vs MultiNest: 0.04 (excellent)
  Dynesty vs UltraNest: 0.01 (excellent)
  MultiNest vs UltraNest: 0.03 (excellent)

log10(f_sat) [dex]:
  Emcee vs Dynesty:     0.02 (excellent)
  Emcee vs MultiNest:   0.02 (excellent)
  Emcee vs UltraNest:   0.06 (excellent)
  Dynesty vs MultiNest: 0.04 (excellent)
  Dynesty vs UltraNest: 0.03 (excellent)
  MultiNest vs UltraNest: 0.08 (excellent)

t_sat [Gyr]:
  Emcee vs Dynesty:     0.10 (excellent)
  Emcee vs MultiNest:   0.04 (excellent)
  Emcee vs UltraNest:   0.10 (excellent)
  Dynesty vs MultiNest: 0.06 (excellent)
  Dynesty vs UltraNest: 0.01 (excellent)
  MultiNest vs UltraNest: 0.06 (excellent)

Age [Gyr]:
  Emcee vs Dynesty:     0.02 (excellent)
  Emcee vs MultiNest:   0.06 (excellent)
  Emcee vs UltraNest:   0.02 (excellent)
  Dynesty vs MultiNest: 0.04 (excellent)
  Dynesty vs UltraNest: 0.00 (excellent)
  MultiNest vs UltraNest: 0.04 (excellent)

beta_XUV:
  Emcee vs Dynesty:     0.05 (excellent)
  Emcee vs MultiNest:   0.06 (excellent)
  Emcee vs UltraNest:   0.03 (excellent)
  Dynesty vs MultiNest: 0.01 (excellent)
  Dynesty vs UltraNest: 0.02 (excellent)
  MultiNest vs UltraNest: 0.03 (excellent)

ALL 30 pairwise comparisons (5 params x 6 pairs) rate as "excellent"
(delta/sigma < 0.50). The maximum delta/sigma across all comparisons
is 0.10, occurring for t_sat (the unconstrained parameter with the
broadest posterior). This demonstrates that all four samplers have
converged to the same posterior distribution.

6.4 Prior vs Posterior Comparison
----------------------------------------------------------------------
How much each parameter is constrained by the data:

  Parameter       Prior                     Posterior (4-sampler avg)     Constraint
  ---------       -----                     -------------------------    ----------
  m_star          0.1945 +/- ~0.0047        0.1940 +/- 0.0041           ~13% narrower
  log10(f_sat)   -2.92  +/- 0.26           -3.053  +/- 0.248            ~5% narrower, shifted
  t_sat           Uniform [0.1, 5.0]        0.859  +/- 0.560            Strongly constrained
  Age             5.75   +/- 1.38            6.124  +/- 1.248            ~10% narrower, shifted
  beta_XUV        1.18   +/- 0.31            1.286  +/- 0.263            ~15% narrower, shifted

  m_star:       Posterior mean shifted by -0.10 sigma from prior.
                Width reduction is modest; mass is mainly constrained
                by the prior (Berta-Thompson+2015).

  log10(f_sat): Posterior shifted -0.51 sigma toward lower saturation
                fractions. The data prefer a slightly lower XUV
                saturation luminosity than the prior (based on
                Wright+2011 scaling).

  t_sat:        The most dramatic constraint. The uniform prior
                covers [0.1, 5.0] Gyr. The posterior peaks near
                0.4 Gyr with sigma ~ 0.56 Gyr, truncated at the
                lower bound. The data strongly prefer a short
                saturation time.

  Age:          Posterior shifted by +0.27 sigma toward older ages.
                Width reduction is moderate. The likelihood weakly
                prefers a slightly older system than the prior
                (Berta-Thompson+2015).

  beta_XUV:     Posterior shifted by +0.34 sigma toward steeper
                XUV decay. Width reduced by ~15%. The data prefer
                a decay index slightly above the prior mean.


======================================================================
7. METHODOLOGY NOTES
======================================================================

7.1 Mean-Only Surrogate for Emcee
----------------------------------------------------------------------
The emcee sampler uses return_var=False when creating the cached
surrogate likelihood (line 473-474 of gj1132_alabi.py):

    fnSurrogate = sm.create_cached_surrogate_likelihood(
        iter=iActiveIterations, return_var=False)

This produces a mean-only GP prediction (no variance estimate).
The ffnSafeSurrogate wrapper then clamps non-finite and positive
values to -1e4.

The variance-aware alternative (ffnVarianceAwareSurrogate) was
implemented but not used for emcee because:
  (a) Emcee's DE/Snooker moves are designed for MCMC exploration
      within the high-probability region. Walkers initialized at the
      MAP point stay within the well-trained GP region.
  (b) The variance-aware wrapper adds computational overhead (two GP
      predictions per evaluation instead of one).
  (c) The additional variance check was found to be overly
      conservative, sometimes rejecting valid parameter combinations
      near the edges of the training distribution.

The nested samplers (dynesty, MultiNest, UltraNest) also use mean-only
surrogates with ffnSafeSurrogate clamping. The floor prevents the GP
from extrapolating wildly in untrained regions that nested samplers
must explore when integrating over the full prior volume.

7.2 Likelihood Floor: -1e2 vs -1e10
----------------------------------------------------------------------
The likelihood function returns -1e2 for failed runs (line 236):
    return -1e2

A previous version used -1e10. The change was motivated by:

  (a) GP training quality: With ~77% of training points at the floor
      value, an extreme floor (-1e10) creates a bimodal training
      distribution with extreme dynamic range. The GP's MinMaxScaler
      would map the valid range [-50, 0] to a tiny fraction [0.99999,
      1.0] of the scaled interval, destroying predictive power.

  (b) Nested sampler stability: Dynesty and UltraNest must evaluate
      the likelihood across the full prior volume. When the GP
      extrapolates, extreme floor values create artificial likelihood
      gradients that trap the samplers.

  (c) Physical justification: The worst-case physical log-likelihood
      within the parameter bounds is approximately -50 (when model
      predictions are many sigma away from observations). A floor of
      -100 is already 50 units below any physical solution, ensuring
      it never competes with valid likelihood evaluations.

7.3 pool_method="fork" Requirement
----------------------------------------------------------------------
The SurrogateModel is initialized with pool_method="fork" instead of
the default "forkserver":

    sm = SurrogateModel(..., pool_method="fork")

This is required because:

  (a) The VPLanet Python wrapper at /usr/local/bin/vplanet creates
      child processes via subprocess.Popen. When multiprocessing uses
      "forkserver", the server process does not inherit the parent's
      file descriptors, leading to broken pipe errors when VPLanet
      tries to communicate with its child C process.

  (b) The "fork" method creates exact copies of the parent process,
      preserving all file descriptors, environment variables, and
      module state. This is essential for the vplanet_inference
      wrapper to function correctly in the multiprocessing pool.

  (c) The environment variables OMP_NUM_THREADS=1, etc. (lines 28-32)
      prevent OpenBLAS/MKL thread contention when multiple forked
      workers are running simultaneously.

  (d) The emcee sampler is run with multi_proc=False because the
      surrogate likelihood is cheap to evaluate (~microseconds per
      call), making multiprocessing overhead counterproductive.

7.4 Numpy 2.0 Compatibility
----------------------------------------------------------------------
numpy 2.0 removed np.trapz in favor of np.trapezoid. A search of
the alabi codebase shows this has been addressed (no remaining calls
to np.trapz were found). The fix was applied as part of the general
alabi maintenance to ensure compatibility with numpy >= 2.0.

The MEMORY.md file notes this as a known pitfall: "np.trapz removed
in numpy 2.0 -- use np.trapezoid".

7.5 Thread-Safety Environment Variables
----------------------------------------------------------------------
The script sets five environment variables before any imports to
prevent thread contention in forked workers:

  OMP_NUM_THREADS = "1"
  OPENBLAS_NUM_THREADS = "1"
  MKL_NUM_THREADS = "1"
  NUMEXPR_NUM_THREADS = "1"
  VECLIB_MAXIMUM_THREADS = "1"

Without these, each forked worker would spawn its own thread pool
for linear algebra, potentially oversubscribing the CPU and causing
severe performance degradation (or deadlocks on macOS).


======================================================================
8. OUTPUT FILES
======================================================================

  File                              Size       Description
  ----                              ----       -----------
  surrogate_model.pkl               8.3 MB     Trained GP surrogate (pickle)
  surrogate_model.txt               1.2 KB     GP summary (hyperparameters)
  initial_train_file_sample.npz    24.5 KB     Initial 500 LHS training points
  initial_test_sample.npz          24.5 KB     Initial 500 LHS test points
  emcee_samples.npz                 7.2 MB     181,100 posterior samples (5D)
  dynesty_samples.npz             412.2 KB     10,298 posterior samples
  multinest_samples.npz           534.4 KB     11,112 posterior samples
  ultranest_samples.npz           625.8 KB     13,017 posterior samples
  sampler_comparison.pdf          187.0 KB     Corner plot (4 samplers + MAP)
  pymultinest_custom_*              various    MultiNest run files (7 runs)
  ultranest_custom_iter_500*/       various    UltraNest run directories (3 runs)


======================================================================
9. REPRODUCIBILITY
======================================================================

  Python version:        3.x (tested with 3.9+)
  Key packages:
      alabi              local development version (commit 7f1ef60)
      vplanet_inference  local development version
      emcee              (latest via pip)
      dynesty            (latest via pip)
      pymultinest        (latest via pip, requires MultiNest C library)
      ultranest          (latest via pip)
      george             GP backend
      sklearn            for StandardScaler, MinMaxScaler
      numpy, scipy, matplotlib, corner, vplot, astropy

  VPLanet binary:        native C binary (not Python wrapper)
  Random state:          not fixed (time-based seed in alabi)


======================================================================
10. SUMMARY
======================================================================

All four posterior samplers converge to the same posterior distribution
with excellent pairwise agreement (max delta/sigma = 0.10). The GP
surrogate, trained with 500 initial LHS points and 500 BAPE active
learning iterations, achieves a test MSE of 3.10 on the held-out
set (20.1% improvement over the initial 3.88).

The key result is that despite using fundamentally different sampling
strategies (affine-invariant MCMC, dynamic nested sampling, multi-
ellipsoidal nested sampling, and reactive nested sampling), all four
samplers produce statistically indistinguishable posteriors. This
provides strong evidence that:
  (a) the GP surrogate is an accurate representation of the true
      likelihood surface,
  (b) each sampler has individually converged, and
  (c) the posterior is well-characterized and can be reported with
      confidence.

The total computation time for all four samplers was approximately
11 minutes (wall time), plus ~10 minutes for active learning, plus
~10 minutes for initial LHS evaluation. The entire pipeline from
raw VPLanet calls to four-sampler posterior comparison completes
in approximately 30 minutes on a single machine.

======================================================================
END OF REPORT
======================================================================
